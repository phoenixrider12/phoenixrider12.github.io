<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
    }

    h2 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2HYEBBLV5B"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2HYEBBLV5B');
</script>
        <title>Aryaman Gupta</title>
        <meta property="og:title" content="Factored 3D" />
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:42px"><u>UAV-Guided UGV Navigation</u></span>
    </center>

    <br><br>

    <table align=center width=1000px>
        <tr>
                <center>
                  <a href=''><img class="round" style="width: 700;" src="overview.png"/></a>
                </center>
      </tr>
  </table>
            <br><br>

            <!-- <br> -->
            This is our solution to the DRDO Navigation Challege in Inter IIT Tech Meet 10.0 In the problem statement, we were asked to use a UAV to explore and
            map a mountainous terrain during summer time. After mapping, the UAV must be used to guide a UGV to traverse through the
            terrain during the winter time when roads are covered with snow.<br>
            <br>Key points from the PS<br>
            • The UAV has an IMU, a GPS and a RGBD camera as sensors.<br>
            • The UGV has no sensor.<br>
            • The mapping needs to be completed in the textured world while
            the UGV navigates in the untextured world.
            <br><br>

          <!-- <br> -->

          <table align=center width=180px>
              <tr>
                  <td><span style="font-size:14pt">
                      <a href="https://github.com/phoenixrider12/DRDO-UAV-Guided-UGV-Navigation/tree/master">[Code]</a>
                    </td>

                    <td><span style="font-size:14pt">
                      <a href="https://docs.google.com/presentation/d/1wgCw-pj4WYZ1QgIoMBwp1pqgJasY5RpXskZN9Sr_edU/edit?usp=sharing">[Slides]</a>
                    </td>

                  <td><span style="font-size:14pt">
                      <a href="https://github.com/phoenixrider12/DRDO-UAV-Guided-UGV-Navigation/blob/master/MP_DR_T14_Mid.pdf">[Report]</a>
                    </td>
              </tr>
            </table>
              <br>

                <hr>

         <center><h1>Approach</h1></center>
        <!-- <br> -->
        The project is divided into three parts:<br>
        <ul>
        <li><b>Mapping</b>: We segmented roads based in UAV Camera feed using U-Net, and used frontier exploration to map the
        entire terrain.</li>
        <li><b>UAV Localization and Control</b>: We used Ardupilot coupled with our own control and
        planning algorithms for UAV control. The UAV localization was
        done by filtering and combining the GPS and IMU data.</li>
        <li><b>UGV Localization and Control</b>: For
        localization of UGV, we used Yolov5 on
        the UAV camera feed and localized it relative to the UAV. We used the
        <i>Se2_Navigation</i> and <i>Navfn</i> RIS packages for path planning and UGV control.</li>
        </ul>
        <br>
        
        <hr>

          <center><h1>UAV Localization</h1></center>

          In the problem
          statement, we were provided with GPS and IMU sensors on the UAV. We fused the incoming data from these sensors using <b>Extended Kalman Filter </b>to produce our desired odometry.
          The node <i>ekf_localization</i> in <a href="https://wiki.ros.org/robot_localization"><i>robot_localization</i></a> ROS package provided us with the 15D odometry. Maximum observed error after multiple-goal points turned out to be ±0.5 meters in all three axes.
          <br><br>

          <table align=center width=1000px>
              <tr>
                      <!-- <center> -->
                        <a href=''><img class="round" align="left" style="width: 525;" src="uav_loc.png"/></a>
                        <!-- <figcaption><i>Complete Pipeline of our solution</i></figcaption> -->

                        <a href=''><img class="round" align="right" style="width: 525;" src="uav_loc.gif"/></a>
                        <!-- <figcaption><i>Complete Pipeline of our solution</i></figcaption> -->
                      <!-- </center> -->
            </tr>
        </table>
      <br>
    
<hr>

    <center><h1>Terrain Mapping</h1></center>
    <!-- <br> -->
    <table align=center width=1000px>
        <tr>
                <center>
                  <a href=''><img class="round" style="width: 900;" src="mapping_flow.png"/></a>
                  <!-- <figcaption><i>Comparision of exploration time with different number of bots</i></figcaption> -->
                </center>
        </tr>
    </table>
    <br>

    <h2><b><u>Road Segmentation</u></b></h2>
      We prepared a dataset containing simulated environment images and manually annotated them using <a href="https://www.cvat.ai/">CVAT</a>. Then we fit a <b>UNET</b> to segment the roads in those images. For better training, we used standard augmentation techniques and obtained ~96% accuracy on testing data. More details can be found in our <a href="https://github.com/phoenixrider12/DRDO-UAV-Guided-UGV-Navigation/blob/master/MP_DR_T14_Mid.pdf">report</a>.
    <br><br>
      <table align=center width=1000px>
      <tr>
              <center>
                <a href=''><img class="round" style="width: 900;" src="unet_results.png"/></a>
                <figcaption><i>UNET Segmentation Results</i></figcaption>
              </center>
      </tr>
  </table>
<br>

  <table align=center width=1000px>
    <tr>
            <center>
              <a href=''><img class="round" style="width: 700;" src="mapping_block.png"/></a>
              <figcaption><i>Exploration Pipeline</i></figcaption>
            </center>
    </tr>
</table>
<br><br>
The mapping performed by the UAV has been implemented using
the <a href="http://wiki.ros.org/rtabmap_ros">RTABMAP</a> ROS package. The package uses RGB-D SLAM approach
and create a 2D Occupancy grid map using the 3D
pointcloud values obtained by RGBD camera atop the drone.<br><br>
The package takes in the values from the camera as well as the
odometry, and publishes the projected 2D map of the 3D environment.
<br><br>
Next, we implemented the Frontier Exploration approach using the
<a href="http://wiki.ros.org/explore_lite">Frontier_Exploration</a> ROS package in order to explore the world terrain.
Frontiers are regions on the boundary between open space and
unexplored space. By moving to a new frontier, we can keep building
the map of the environment, until there are no new frontiers left to
detect.

<br><br>

<table align=center width=1000px>
  <tr>
          <!-- <center> -->
            <a href=''><img class="round" align="left" style="width: 500;" src="rtabmap.jpg"/></a>
            <!-- <figcaption><i>Comparision of exploration time with different number of bots</i></figcaption> -->
          <!-- </center> -->
  </tr>

<tr>
  <!-- <center> -->
    <a href=''><img class="round" align="right" style="width: 500;" src="frontier.gif"/></a>
    <!-- <figcaption><i>Comparision of exploration time with different number of bots</i></figcaption> -->
  <!-- </center> -->
</tr>
</table>
<br>
<hr>

    <center><h1>UGV Navigation</h1></center>
    <table align=center width=1000px>
        <tr>
                <center>
                  <a href=''><img class="round" style="width: 900;" src="nav_flow.png"/></a>
                </center>
        </tr>
        <!-- We then iterate using weighted center algorithm for calculating the desired voronoi cells and the positions of each agent. -->
    </table>

    <h2><b><u>UGV Detection and Tracking</u></b></h2>

    <table align=center width=1000px>
      <tr>
              <center>
                <a href=''><img class="round" style="width: 800;" src="ugv_detect_flow.png"/></a>
                <figcaption><i>Detection and Tracking Pipeline</i></figcaption>
              </center>
      </tr>
    </table>
    <br>
    <table align=center width=1000px>
    <tr>
      <!-- <center> -->
        <a href=''><img class="round" align="left" style="width: 500;" src="ugv_detect.png"/></a>
        <!-- <figcaption><i>Comparision of exploration time with different number of bots</i></figcaption> -->
      <!-- </center> -->

      <!-- <center> -->
        <a href=''><img class="round" align="right" style="width: 500;" src="ugv_track.gif"/></a>
        <!-- <figcaption><i>Comparision of exploration time with different number of bots</i></figcaption> -->
      <!-- </center> -->
    </tr>
    </table>
    <br>
    <h2><b><u>UGV Controls</u></b></h2>

    For UGV control Pure Pursuit Controller was used which is a path tracking algorithm.It computes the angular velocity command that moves the robot from its current position to reach some look-ahead point in front of the robot.
    <br><br>
    <table align=center width=1000px>
      <tr>
        <!-- <center> -->
          <a href=''><img class="round" align="left" style="width: 500;" src="ugv_control_block.png"/></a>
          <!-- <figcaption><i>Comparision of exploration time with different number of bots</i></figcaption> -->
        <!-- </center> -->
  
        <!-- <center> -->
          <a href=''><img class="round" align="right" style="width: 500;" src="ugv_control.png"/></a>
          <!-- <figcaption><i>Comparision of exploration time with different number of bots</i></figcaption> -->
        <!-- </center> -->
      </tr>
      </table>
      <br>
      <h2><b><u>UGV Planning</u></b></h2>

      <a href=''><img class="round" align="right" style="width: 300;" src="ugv_planning.png"/></a>
      
      <ul>
      <li><b>Local Planner-OMPL(Open Motion Planning Library)</b> is a collection of state-of-the-art sampling-based motion planning algorithms.</li>
      <li><b>Global Planner-TEB (Timed Elastic Band)</b> locally optimizes the robot's trajectory with respect to trajectory execution time, separation from obstacles and compliance with kinodynamic constraints at runtime.</li>
      </ul>

      We used <a href="https://github.com/leggedrobotics/se2_navigation">se2_navigation</a> ROS package
      which subscribes to the odometry topic of car with respect
      to a suitable odom frame to receive car's odometry w.r.t initial starting position
      and orientation of the drone.<br><br>
      Now when we publish a suitable goal point in the same odom frame, the
      <i>se2 OMPL planner</i> inside this package plans a proper trajectory for the
      car to follow which is a sequential array of poses in the odometry frame.
      <br><br>
      <hr> 
        <table align=center width=1100px>
        <tr>
        <td>
            <left>
                This webpage template was borrowed from some <a href="https://richzhang.github.io/colorization/">colorful folks</a>.
            </left>
        </td>
        </tr>
        </table>

        <br><br>
</body>
</html>
